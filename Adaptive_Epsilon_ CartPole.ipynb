{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0890995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495252b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3335d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "discretization_segments = (2,2,2,2)\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "q_table = np.zeros(discretization_segments + (NUM_ACTIONS,))\n",
    "\n",
    "cart_vel_min = -1.25\n",
    "cart_vel_max = 1.25\n",
    "pole_ang_vel_min = -np.radians(45)\n",
    "pole_ang_vel_max = np.radians(45)\n",
    "\n",
    "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "state_bounds[1] = [cart_vel_min, cart_vel_max]\n",
    "state_bounds[3] = [pole_ang_vel_min, pole_ang_vel_max]\n",
    "\n",
    "EXPLORE_RATE_MIN = 0.01\n",
    "LEARNING_RATE_MIN = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explore_rate(t):\n",
    "    return max(EXPLORE_RATE_MIN, min(1, 1.0-math.log10((t+1)/25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c878dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(t):\n",
    "    return max(LEARNING_RATE_MIN, min(0.5, 1.0-math.log10((t+1)/25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e272d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, explore_rate):\n",
    "    if random.random() < explore_rate:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(q_table[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238bc866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_bucket(state):\n",
    "    \n",
    "    bucket_indices = []\n",
    "    \n",
    "    for i in range(len(state)):\n",
    "        if state[i] <= state_bounds[i][0]:\n",
    "            bucket_index = 0\n",
    "            \n",
    "        elif state[i] >= state_bounds[i][1]:\n",
    "            bucket_index = discretization_segments[i] - 1\n",
    "        \n",
    "        else:\n",
    "            bound_width = state_bounds[i][1] - state_bounds[i][0]\n",
    "            \n",
    "            offset = (discretization_segments[i] - 1) * state_bounds[i][0] / bound_width\n",
    "            scaling = (discretization_segments[i] - 1) / bound_width\n",
    "            \n",
    "            bucket_index = int(round(scaling * state[i] - offset))\n",
    "        \n",
    "        bucket_indices.append(bucket_index)\n",
    "    \n",
    "    return tuple(bucket_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate():\n",
    "    learning_rate = get_learning_rate(0)\n",
    "    explore_rate = get_explore_rate(0)\n",
    "    \n",
    "    discount_factor = 0.99\n",
    "    num_streaks = 0\n",
    "    reward_sum_per_episode = []\n",
    "    for episode in range(100):\n",
    "        episode_rewards = []\n",
    "        (observ,_) = env.reset()\n",
    "        \n",
    "        state_0 = state_to_bucket(observ)\n",
    "        \n",
    "        for t in range(250):\n",
    "            \n",
    "            env.render()\n",
    "            \n",
    "            action = select_action(state_0, explore_rate)\n",
    "            \n",
    "            (observ, reward, done, truncated,_) = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "            state = state_to_bucket(observ)\n",
    "            \n",
    "            best_q = np.amax(q_table[state])\n",
    "            \n",
    "            q_table[state_0 + (action,)] += learning_rate * (reward + discount_factor*(best_q) - q_table[state_0 + (action,)])\n",
    "            \n",
    "            \n",
    "            state_0 = state\n",
    "            \n",
    "            # print(\"\\nEpisode = %d\" % episode)\n",
    "            # print(\"t = %d\" % t)\n",
    "            # print(\"Action: %d\" % action)\n",
    "            # print(\"State: %s\" %str(state))\n",
    "            # print(\"Reward: %f\" % np.sum(episode_rewards))\n",
    "            # print(\"Best Q: %f\" % best_q)\n",
    "            # print(\"Explore rate: %f\" % explore_rate)\n",
    "            # print(\"Learning rate: %f\" % learning_rate)\n",
    "            # print(\"Streaks: %d\" %num_streaks)\n",
    "            \n",
    "            # print(\"\")\n",
    "            \n",
    "            if done:\n",
    "                print(\"Episode %d finished after %f time steps\" % (episode, t))\n",
    "                reward_sum_per_episode.append(np.sum(episode_rewards))\n",
    "                if (t >= 199):\n",
    "                    num_streaks += 1\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                break\n",
    "            \n",
    "            if num_streaks > 120:\n",
    "                break\n",
    "            explore_rate = get_explore_rate(episode)\n",
    "            learning_rate = get_learning_rate(episode)\n",
    "    return reward_sum_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29734388",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb62853",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe5c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(reward, color=\"green\", linewidth=1)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Sum of Rewards in Episode\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
