{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d37d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")  # , render_mode = \"human\"\n",
    "\n",
    "alpha = 0.995\n",
    "dis_factor = 0.9\n",
    "epsilon = 0.05\n",
    "max_number_episodes = 5000\n",
    "\n",
    "discretization_segments = (7,7,7,7)\n",
    "\n",
    "cart_vel_min = -1.25\n",
    "cart_vel_max = 1.25\n",
    "pole_ang_vel_min = -np.radians(45)\n",
    "pole_ang_vel_max = np.radians(45)\n",
    "\n",
    "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "state_bounds[1] = [cart_vel_min, cart_vel_max]\n",
    "state_bounds[3] = [pole_ang_vel_min, pole_ang_vel_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fcf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        alpha,\n",
    "        dis_factor,\n",
    "        epsilon,\n",
    "        max_episodes,\n",
    "        discretization_segments,\n",
    "        state_bounds,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.dis_factor = dis_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.max_episodes = max_episodes\n",
    "        self.discretization_segments = discretization_segments\n",
    "        self.state_bounds = state_bounds\n",
    "        self.action = self.env.action_space.n\n",
    "        self.reward_sum_per_episode = []\n",
    "        self.best_action_value_per_episode = []\n",
    "        self.mean_reward = []\n",
    "        self.q_table = np.random.uniform(\n",
    "            low=0, high=1, size=(self.discretization_segments + (self.action,))\n",
    "        )\n",
    "        # If you want the initial q table to be zero, comment the line above this one \n",
    "        # and uncomment the line below\n",
    "        # self.q_table = np.zeros(self.discretization_segments + (self.action,))\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        if (\n",
    "            np.random.random() < self.epsilon\n",
    "            or self.q_table[state][0] == self.q_table[state][1]\n",
    "        ):\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[state])\n",
    "        return action\n",
    "    \n",
    "    def discretize_state(self, state):\n",
    "        state_indices = []\n",
    "        for i in range(len(state)):\n",
    "            if state[i] <= self.state_bounds[i][0]:\n",
    "                indexed_state = 0\n",
    "            elif state[i] >= self.state_bounds[i][1]:\n",
    "                indexed_state = self.discretization_segments[i] - 1\n",
    "            else:\n",
    "                bound_width = self.state_bounds[i][1] - self.state_bounds[i][0]\n",
    "\n",
    "                offset = (\n",
    "                    (self.discretization_segments[i] - 1)\n",
    "                    * self.state_bounds[i][0]\n",
    "                    / bound_width\n",
    "                )\n",
    "                scaling = (self.discretization_segments[i] - 1) / bound_width\n",
    "\n",
    "                indexed_state = int(round(scaling * state[i] - offset))\n",
    "\n",
    "            state_indices.append(indexed_state)\n",
    "\n",
    "        return tuple(state_indices)\n",
    "\n",
    "    def train(self):\n",
    "        total_reward = 0\n",
    "        for episode in range(self.max_episodes):\n",
    "            episode_rewards = 0\n",
    "            epsiode_action_values = []\n",
    "            (state, _) = self.env.reset()\n",
    "            indexed_state = self.discretize_state(state)\n",
    "            terminal_state = False\n",
    "            \n",
    "            print(\"\\nEpisode = %d\" % episode)\n",
    "            \n",
    "            for t in range(750):\n",
    "                self.env.render()\n",
    "                action = self.select_action(indexed_state)\n",
    "\n",
    "                (next_state, reward, terminal_state, truncated, _) = self.env.step(action)\n",
    "                episode_rewards += reward\n",
    "                total_reward += reward\n",
    "                next_indexed_state = self.discretize_state(next_state)\n",
    "\n",
    "                next_best_action = np.amax(self.q_table[next_indexed_state])\n",
    "\n",
    "                epsiode_action_values.append(self.q_table[next_indexed_state])\n",
    "\n",
    "                td_target = reward + (self.dis_factor * next_best_action)\n",
    "\n",
    "                td_error = td_target - self.q_table[indexed_state + (action,)]\n",
    "\n",
    "                self.q_table[indexed_state + (action,)] = self.q_table[\n",
    "                    indexed_state + (action,)\n",
    "                ] + self.alpha * (td_error)\n",
    "\n",
    "                indexed_state = next_indexed_state\n",
    "\n",
    "                if terminal_state or truncated:\n",
    "                    self.reward_sum_per_episode.append(episode_rewards)\n",
    "                    self.best_action_value_per_episode.append(\n",
    "                        np.max(epsiode_action_values)\n",
    "                    )\n",
    "                    print(\"Episode %d finished after %f time steps\" % (episode, t))\n",
    "                    print(\"Reward: %f\" % episode_rewards)\n",
    "                    break\n",
    "            if episode == 0:\n",
    "                self.mean_reward.append(total_reward)\n",
    "            else:\n",
    "                self.mean_reward.append(total_reward / episode)\n",
    "        print(\"Average Reward: {}\".format(total_reward / episode))\n",
    "        self.env.close()\n",
    "        \n",
    "    def simulate(self,simulation_iterations):\n",
    "        env2 = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "        total_reward = 0\n",
    "        mean = []\n",
    "        for episode in range(simulation_iterations):\n",
    "            (state, _) = env2.reset()\n",
    "            indexed_state = self.discretize_state(state)\n",
    "            terminal_state = False\n",
    "            for t in range(500):\n",
    "                env2.render()\n",
    "                action = self.select_action(indexed_state)\n",
    "\n",
    "                (next_state, reward, terminal_state, truncated, _) = env2.step(action)\n",
    "                total_reward += reward\n",
    "                \n",
    "                next_indexed_state = self.discretize_state(next_state)\n",
    "\n",
    "                next_best_action = np.amax(self.q_table[next_indexed_state])\n",
    "\n",
    "\n",
    "                indexed_state = next_indexed_state\n",
    "\n",
    "                if terminal_state or truncated:\n",
    "                    break\n",
    "            if episode == 0:\n",
    "                mean.append(total_reward)\n",
    "            else:\n",
    "                mean.append(total_reward / episode)\n",
    "        print(\"Average Reward: {}\".format(total_reward / episode))            \n",
    "        env2.close()\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c081d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = QLearning(\n",
    "     env,\n",
    "        alpha,\n",
    "        dis_factor,\n",
    "        epsilon,\n",
    "        max_number_episodes,\n",
    "        discretization_segments,\n",
    "        state_bounds\n",
    "    )\n",
    "Q.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225862b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(Q.reward_sum_per_episode, color=\"green\", linewidth=1)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Sum of Rewards in Episode While Training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef023925",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(Q.mean_reward, color=\"black\", linewidth=1),\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Mean Reward While Training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d7d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5)),\n",
    "plt.plot(Q.best_action_value_per_episode, color=\"blue\", linewidth=1)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Best Action Value in Each Episode While Training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953adf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q.q_table)\n",
    "print(np.max(Q.q_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e173f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = Q.simulate(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d551d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(mean, color=\"black\", linewidth=1),\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Mean reward During Simulation\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
